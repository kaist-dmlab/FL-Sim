{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Test\n",
    "##### https://woolulu.tistory.com/category/Tensorflow/Support%20Vector%20Machine\n",
    "##### https://medium.com/cs-note/tensorflow-ch4-support-vector-machines-c9ad18878c76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1006 19:24:36.482066 140145767819008 deprecation.py:323] From /home/dmlab/prosopher/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (10000, 3072)\n",
      "0,0.8555577993392944,0.6000000238418579\n",
      "50,0.7705511450767517,0.7020000219345093\n",
      "100,0.615459144115448,0.7545999884605408\n",
      "150,0.6015509963035583,0.7613999843597412\n",
      "200,0.5836949944496155,0.7674999833106995\n",
      "250,0.5728937983512878,0.7700999975204468\n",
      "300,0.5584739446640015,0.7771000266075134\n",
      "350,0.5655651092529297,0.7753000259399414\n",
      "400,0.5490298271179199,0.781000018119812\n",
      "450,0.5702677965164185,0.7753999829292297\n",
      "500,0.5617369413375854,0.7782999873161316\n",
      "550,0.5505491495132446,0.7821999788284302\n",
      "600,0.5583293437957764,0.7814000248908997\n",
      "650,0.5497826933860779,0.7835000157356262\n",
      "700,0.5454190969467163,0.7850000262260437\n",
      "750,0.5351250767707825,0.7883999943733215\n",
      "800,0.5536011457443237,0.784500002861023\n",
      "850,0.5317941308021545,0.7890999913215637\n",
      "900,0.5577540397644043,0.7839999794960022\n",
      "950,0.5423465371131897,0.7870000004768372\n",
      "1000,0.5167508721351624,0.794700026512146\n",
      "1050,0.5468797087669373,0.7868000268936157\n",
      "1100,0.558967113494873,0.7838000059127808\n",
      "1150,0.561177134513855,0.7831000089645386\n",
      "1200,0.5603949427604675,0.78329998254776\n",
      "1250,0.5598225593566895,0.7831000089645386\n",
      "1300,0.5656589865684509,0.7803999781608582\n",
      "1350,0.5660170912742615,0.7806000113487244\n",
      "1400,0.5623342990875244,0.7825999855995178\n",
      "1450,0.581430196762085,0.7753999829292297\n",
      "1500,0.5672285556793213,0.7817999720573425\n",
      "1550,0.5270835161209106,0.7930999994277954\n",
      "1600,0.5215578675270081,0.7940999865531921\n",
      "1650,0.5385120511054993,0.7893000245094299\n",
      "1700,0.5480359196662903,0.7872999906539917\n",
      "1750,0.5448744297027588,0.788100004196167\n",
      "1800,0.5077998042106628,0.8001000285148621\n",
      "1850,0.5098476409912109,0.7990999817848206\n",
      "1900,0.5308021306991577,0.7918000221252441\n",
      "1950,0.5127130150794983,0.7975999712944031\n",
      "2000,0.5497406125068665,0.7871999740600586\n",
      "2050,0.5620648264884949,0.782800018787384\n",
      "2100,0.5667338967323303,0.781000018119812\n",
      "2150,0.5745808482170105,0.7786999940872192\n",
      "2200,0.5819647312164307,0.7768999934196472\n",
      "2250,0.5460460186004639,0.7878000140190125\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# iris = load_iris()\n",
    "# x_vals = iris.data[:, [0,3]]\n",
    "# y_vals = np.array([1 if y==0 else -1 for y in iris.target])\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_vals, y_vals, random_state=0, stratify=y_vals)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import fl_data\n",
    "\n",
    "ops.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "DATA_NAME = 'cifar10' # mnist-o / mnist-f / cifar-10\n",
    "train_data, test_data = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_dataBy1Nid = fl_data.preprocess('svm', DATA_NAME, train_data, True)\n",
    "test_dataBy1Nid = fl_data.preprocess('svm', DATA_NAME, test_data, True)\n",
    "print(train_dataBy1Nid[0]['x'].shape, test_dataBy1Nid[0]['x'].shape)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(shape=[None, 3072], dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "\n",
    "# create Variable\n",
    "A = tf.Variable(tf.zeros(shape=[3072, 1], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros(shape=[1], dtype=tf.float32))\n",
    "\n",
    "# formula (y = ax - b)\n",
    "formula = tf.matmul(x, A) - b\n",
    "\n",
    "# declare loss function\n",
    "# Loss = summation(max(0, 1 - pred*actual)) + alpha * L2_norm(A)^2\n",
    "y_reshaped = tf.reshape(y, [-1, 1])\n",
    "loss = tf.reduce_mean(tf.maximum(0.0, 1.0 - formula * tf.cast(y_reshaped, tf.float32)))\n",
    "\n",
    "y_hat = tf.squeeze(tf.cast(tf.sign(formula), tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_hat, y), tf.float32))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "# initialize variable\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "x_train = train_dataBy1Nid[0]['x'][0:2000]\n",
    "y_train = train_dataBy1Nid[0]['y'][0:2000]\n",
    "x_test = test_dataBy1Nid[0]['x'][0:10000]\n",
    "y_test = test_dataBy1Nid[0]['y'][0:10000]\n",
    "\n",
    "# x = x_train\n",
    "# y = y_train.reshape(-1, 1)\n",
    "for t in np.arange(100000):\n",
    "    sess.run(train_op, feed_dict={x: x_train, y:y_train})\n",
    "    if t % 50 == 0:\n",
    "#        vs = sess.run(tf.concat([ tf.reshape(layer, [-1]) for layer in tf.get_collection(tf.GraphKeys.VARIABLES) ], axis=0))\n",
    "#        numVars = vs.shape ; print(numVars)\n",
    "        (loss_, accuracy_) = sess.run((loss, accuracy), feed_dict={x: x_test, y:y_test})\n",
    "        print('{},{},{}'.format(t, loss_, accuracy_))\n",
    "    if accuracy_ >= 0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian SVM Test\n",
    "#### https://github.com/mjc13813759744/machine-Learning/blob/master/non_linear%20SVM(RBF)%20by%20tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b55649784a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "import fl_data\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "DATA_NAME = 'cifar10' # mnist-o / mnist-f / cifar-10\n",
    "train_data, test_data = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_dataBy1Nid = fl_data.preprocess('svm', DATA_NAME, train_data, True)\n",
    "test_dataBy1Nid = fl_data.preprocess('svm', DATA_NAME, test_data, True)\n",
    "print(train_dataBy1Nid[0]['x'].shape, test_dataBy1Nid[0]['x'].shape)\n",
    "\n",
    "x_train = train_dataBy1Nid[0]['x']\n",
    "y_train = train_dataBy1Nid[0]['y']\n",
    "x_test = test_dataBy1Nid[0]['x']\n",
    "y_test = test_dataBy1Nid[0]['y']\n",
    "\n",
    "# (x_vals_, y_vals_) = datasets.make_circles(n_samples=350, factor=.5, noise=.1)\n",
    "# y_vals_ = np.array([1 if y==1 else -1 for y in y_vals_])\n",
    "# print(x_vals_.shape, y_vals.shape)\n",
    "\n",
    "x = tf.placeholder(shape=[None, 3072], dtype=tf.float32)\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "y_reshaped = tf.reshape(y, [-1, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal(shape=[1, 3072]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1, 1]))\n",
    "\n",
    "#compute Gaussian Kernel \n",
    "gamma = tf.constant(-10.0)\n",
    "\n",
    "#dist = tf.reduce_sum(tf.square(x), 1)\n",
    "#dist = tf.reshape(dist, [-1, 1])\n",
    "#sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x, tf.transpose(x)))), tf.transpose(dist))\n",
    "dist = tf.reduce_sum(tf.square(tf.transpose(x)), 1)\n",
    "dist = tf.reshape(dist, [-1, 1])\n",
    "sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(tf.transpose(x), x))), tf.transpose(dist))\n",
    "my_kernel = tf.exp(tf.multiply(gamma, sq_dists))\n",
    "output = tf.add(tf.matmul(W, my_kernel), b)\n",
    "print(tf.shape(output))\n",
    "\n",
    "y_float = tf.cast(y, tf.float32)\n",
    "loss = tf.reduce_mean(tf.maximum(0.,tf.subtract(1., tf.multiply(output, y_float)))) + tf.matmul(W, tf.transpose(W))\n",
    "\n",
    "# x_trainfloat = x #tf.to_float(x_train)\n",
    "\n",
    "# pred_dist = tf.reduce_sum(tf.square(x_trainfloat), 1)\n",
    "# pred_dist = tf.reshape(pred_dist, [-1, 1])\n",
    "# pred_sq_dists = tf.add(tf.subtract(pred_dist, tf.multiply(2., tf.matmul(x_trainfloat, tf.transpose(x_trainfloat)))), tf.transpose(pred_dist))\n",
    "# pred_kernel = tf.exp(tf.multiply(gamma, pred_sq_dists))\n",
    "\n",
    "# predict_train = tf.add(tf.matmul(W, my_kernel), b)\n",
    "# prediction = tf.sign(predict_train)\n",
    "y_hat = tf.squeeze(tf.cast(tf.sign(output), tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_hat, y), tf.float32))\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def next_batch(batchSize, x, y):\n",
    "    idx = np.arange(0 , len(x))\n",
    "    np.random.shuffle(idx)\n",
    "    x_ = [x[i] for i in idx[:batchSize]]\n",
    "    y_ = [y[i] for i in idx[:batchSize]]\n",
    "    return x_, y_\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10000):\n",
    "    trainBatch = next_batch(BATCH_SIZE, x_train, y_train)\n",
    "    sess.run(train_op, feed_dict={x:trainBatch[0], y:trainBatch[1]})\n",
    "    \n",
    "#     if epoch % 1000 == 0:\n",
    "#         (loss_, acc_) = sess.run((loss, accuracy), feed_dict={x:x_train, y:y_train})\n",
    "#         print(epoch, loss_, acc_)\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        numTestIters = int(10000/BATCH_SIZE)\n",
    "        losses_ = [] ; accs_ = [] ; idxBegin = 0 ; idxEnd = 0\n",
    "        for i in range(numTestIters):\n",
    "            idxEnd += BATCH_SIZE\n",
    "            sampleBatch_x = x_test[idxBegin:idxEnd]\n",
    "            sampleBatch_y = y_test[idxBegin:idxEnd]\n",
    "            (loss_, acc_) = sess.run((loss, accuracy), feed_dict={x: sampleBatch_x, y: sampleBatch_y})\n",
    "            losses_.append(loss_)\n",
    "            accs_.append(acc_)\n",
    "            idxBegin = idxEnd\n",
    "        (loss_, acc_) = np.mean(losses_), np.mean(accs_)\n",
    "        #vs = sess.run(tf.concat([ tf.reshape(layer, [-1]) for layer in tf.get_collection(tf.GraphKeys.VARIABLES) ], axis=0))\n",
    "        #numVars = vs.shape ; print(numVars)\n",
    "        print(epoch, loss_, acc_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9-layer CNN Test\n",
    "##### https://www.tensorflow.org/tutorials/images/deep_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "import fl_data\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(data):\n",
    "    x = np.array([ x / 255.0 for x in data[0] ], dtype=np.float32)\n",
    "    dataY = data[1].flatten() # cifar10 의 경우 flatten 필요\n",
    "    y = np.array(dataY, dtype=np.int32)\n",
    "    return np.array([ { 'x': x, 'y': y } ])\n",
    "\n",
    "def next_batch(batchSize, x, y):\n",
    "    idx = np.arange(0 , len(x))\n",
    "    np.random.shuffle(idx)\n",
    "    x_ = [x[i] for i in idx[:batchSize]]\n",
    "    y_ = [y[i] for i in idx[:batchSize]]\n",
    "    return x_, y_\n",
    "\n",
    "def build_CNN_classifier(x):\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    \n",
    "    # 첫번째 convolutional layer - 하나의 grayscale 이미지를 64개의 특징들(feature)으로 맵핑(maping)합니다.\n",
    "    W_conv1 = tf.get_variable('W_conv1', dtype=tf.float32, initializer=tf.truncated_normal(shape=[5, 5, 1, 64], stddev=5e-2))\n",
    "    b_conv1 = tf.get_variable('b_conv1', dtype=tf.float32, initializer=tf.constant(0.1, shape=[64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "\n",
    "    # 첫번째 Pooling layer\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    norm1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # 두번째 convolutional layer - 32개의 특징들(feature)을 64개의 특징들(feature)로 맵핑(maping)합니다.\n",
    "    W_conv2 = tf.get_variable('W_conv2', dtype=tf.float32, initializer=tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "    b_conv2 = tf.get_variable('b_conv2', dtype=tf.float32, initializer=tf.constant(0.1, shape=[64]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(norm1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "    norm2 = tf.nn.lrn(h_conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "\n",
    "    # 두번째 pooling layer.\n",
    "    h_pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #   h_pool2_flatten = tf.reshape(h_pool2, [-1, 2048])\n",
    "    h_pool2_flatten = tf.keras.layers.Flatten()(h_pool2)\n",
    "\n",
    "    #   # 세번째 convolutional layer\n",
    "    #   W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "    #   b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    #   h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "\n",
    "    #   # 네번째 convolutional layer\n",
    "    #   W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    #   b_conv4 = tf.Variable(tf.constant(0.1, shape=[128])) \n",
    "    #   h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4)\n",
    "\n",
    "    #   # 다섯번째 convolutional layer\n",
    "    #   W_conv5 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "    #   b_conv5 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    #   h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5)\n",
    "    #   h_conv5_flat = tf.reshape(h_conv5, [-1, 8*8*128])\n",
    "\n",
    "\n",
    "    # Fully Connected Layer 1 - 2번의 downsampling 이후에, 우리의 32x32 이미지는 8x8x128 특징맵(feature map)이 됩니다.\n",
    "    # 이를 384개의 특징들로 맵핑(maping)합니다.\n",
    "\n",
    "    W_fc1 = tf.get_variable('W_fc1', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3136, 512], stddev=5e-2))\n",
    "    b_fc1 = tf.get_variable('b_fc1', dtype=tf.float32, initializer=tf.constant(0.1, shape=[512]))\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flatten, W_fc1) + b_fc1)\n",
    "\n",
    "    #   W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 192], stddev=5e-2))\n",
    "    #   b_fc2 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "    #   h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)  \n",
    "\n",
    "    # Fully Connected Layer 2 - 384개의 특징들(feature)을 10개의 클래스-airplane, automobile, bird...-로 맵핑(maping)합니다.\n",
    "    W_fc3 = tf.get_variable('W_fc3', dtype=tf.float32, initializer=tf.truncated_normal(shape=[512, 10], stddev=5e-2))\n",
    "    b_fc3 = tf.get_variable('b_fc3', dtype=tf.float32, initializer=tf.constant(0.1, shape=[10]))\n",
    "    logits = tf.matmul(h_fc1,W_fc3) + b_fc3\n",
    "    y_prob = tf.nn.softmax(logits)\n",
    "    y_hat = tf.cast(tf.argmax(y_prob, 1), tf.int32)\n",
    "    return y_hat, logits\n",
    "\n",
    "trainData, testData = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "trainData_by1Nid = fl_data.preprocess('cnn', 'mnist-o', trainData, False)\n",
    "testData_by1Nid = fl_data.preprocess('cnn', 'mnist-o', testData, False)\n",
    "x_train = trainData_by1Nid[0]['x']\n",
    "y_train = trainData_by1Nid[0]['y']\n",
    "x_test = testData_by1Nid[0]['x']\n",
    "y_test = testData_by1Nid[0]['y']\n",
    "print(x_train.shape)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28])\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "\n",
    "y_hat, logits = build_CNN_classifier(x)\n",
    "y_onehot = tf.one_hot(y, 10)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=logits))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_hat, y), tf.float32))\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    lr_ = 0.01\n",
    "    for t in range(100000):\n",
    "        trainBatch = next_batch(BATCH_SIZE, x_train, y_train)\n",
    "        testBatch = next_batch(BATCH_SIZE, x_test, y_test)\n",
    "        sess.run(train_op, feed_dict={lr: lr_, x: trainBatch[0], y: trainBatch[1]})\n",
    "        \n",
    "        if t % 390 == 0:\n",
    "            lr_ *= 0.99\n",
    "            losses_ = [] ; accs_ = []\n",
    "            for _ in range(78):\n",
    "                (sampleBatch_x, sampleBatch_y) = next_batch(BATCH_SIZE, x_test, y_test)\n",
    "                (loss_, acc_) = sess.run((loss, accuracy), feed_dict={x: sampleBatch_x, y: sampleBatch_y})\n",
    "                losses_.append(loss_)\n",
    "                accs_.append(acc_)\n",
    "            print(\"%d\\t%.4f\\t%.3f\\t%.3f\" % (t, lr_, np.mean(losses_), np.mean(accs_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with CIFAR10\n",
    "#### http://solarisailab.com/archives/2325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "import fl_data\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(data):\n",
    "    x = np.array([ x / 255.0 for x in data[0] ], dtype=np.float32)\n",
    "    dataY = data[1].flatten() # cifar10 의 경우 flatten 필요\n",
    "    y = np.array(dataY, dtype=np.int32)\n",
    "    return np.array([ { 'x': x, 'y': y } ])\n",
    "\n",
    "def next_batch(batchSize, x, y):\n",
    "    idx = np.arange(0 , len(x))\n",
    "    np.random.shuffle(idx)\n",
    "    x_ = [x[i] for i in idx[:batchSize]]\n",
    "    y_ = [y[i] for i in idx[:batchSize]]\n",
    "    return x_, y_\n",
    "\n",
    "# CNN 모델을 정의합니다. \n",
    "def build_CNN_classifier(x):\n",
    "    # 첫번째 convolutional layer - 하나의 grayscale 이미지를 64개의 특징들(feature)으로 맵핑(maping)합니다.\n",
    "#     W_conv1 = tf.get_variable('W_conv1', dtype=tf.float32, initializer=tf.truncated_normal(shape=[5, 5, 3, 64], stddev=5e-2))\n",
    "#     b_conv1 = tf.get_variable('b_conv1', dtype=tf.float32, initializer=tf.constant(0.1, shape=[64]))\n",
    "    W_conv1 = tf.get_variable('W_conv1', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 3, 64], stddev=5e-2))\n",
    "    b_conv1 = tf.get_variable('b_conv1', dtype=tf.float32, initializer=tf.constant(0.1, shape=[64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='VALID') + b_conv1)\n",
    "    \n",
    "    # 첫번째 Pooling layer\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # 두번째 convolutional layer - 32개의 특징들(feature)을 64개의 특징들(feature)로 맵핑(maping)합니다.\n",
    "#     W_conv2 = tf.get_variable('W_conv2', dtype=tf.float32, initializer=tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "#     b_conv2 = tf.get_variable('b_conv2', dtype=tf.float32, initializer=tf.constant(0.1, shape=[64]))\n",
    "    W_conv2 = tf.get_variable('W_conv2', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "    b_conv2 = tf.get_variable('b_conv2', dtype=tf.float32, initializer=tf.constant(0.1, shape=[128]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='VALID') + b_conv2)\n",
    "    \n",
    "    # 두번째 pooling layer.\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 세번째 convolutional layer\n",
    "#     W_conv3 = tf.get_variable('W_conv3', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "#     b_conv3 = tf.get_variable('b_conv3', dtype=tf.float32, initializer=tf.constant(0.1, shape=[128]))\n",
    "    W_conv3 = tf.get_variable('W_conv3', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 128, 256], stddev=5e-2))\n",
    "    b_conv3 = tf.get_variable('b_conv3', dtype=tf.float32, initializer=tf.constant(0.1, shape=[256]))\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='VALID') + b_conv3)\n",
    "\n",
    "    # 네번째 convolutional layer\n",
    "#     W_conv4 = tf.get_variable('W_conv4', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "#     b_conv4 = tf.get_variable('b_conv4', dtype=tf.float32, initializer=tf.constant(0.1, shape=[128]))\n",
    "    W_conv4 = tf.get_variable('W_conv4', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 256, 256], stddev=5e-2))\n",
    "    b_conv4 = tf.get_variable('b_conv4', dtype=tf.float32, initializer=tf.constant(0.1, shape=[256]))\n",
    "    h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='VALID') + b_conv4)\n",
    "\n",
    "    # 다섯번째 convolutional layer\n",
    "#     W_conv5 = tf.get_variable('W_conv5', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "#     b_conv5 = tf.get_variable('b_conv5', dtype=tf.float32, initializer=tf.constant(0.1, shape=[128]))\n",
    "    W_conv5 = tf.get_variable('W_conv5', dtype=tf.float32, initializer=tf.truncated_normal(shape=[3, 3, 256, 256], stddev=5e-2))\n",
    "    b_conv5 = tf.get_variable('b_conv5', dtype=tf.float32, initializer=tf.constant(0.1, shape=[256]))\n",
    "    h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='VALID') + b_conv5)\n",
    "\n",
    "    # Fully Connected Layer 1 - 2번의 downsampling 이후에, 우리의 32x32 이미지는 8x8x128 특징맵(feature map)이 됩니다.\n",
    "    # 이를 384개의 특징들로 맵핑(maping)합니다.\n",
    "#     W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 128, 384], stddev=5e-2))\n",
    "#     b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape=[1 * 1 * 256, 128], stddev=5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "\n",
    "    h_conv5_flat = tf.reshape(h_conv5, [-1, 1 * 1 * 256])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout - 모델의 복잡도를 컨트롤합니다. 특징들의 co-adaptation을 방지합니다.\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "    # Fully Connected Layer 2 - 384개의 특징들(feature)을 10개의 클래스-airplane, automobile, bird...-로 맵핑(maping)합니다.\n",
    "#     W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
    "#     b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape=[128, 10], stddev=5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "    y_prob = tf.nn.softmax(logits)\n",
    "    y_hat = tf.cast(tf.argmax(y_prob, 1), tf.int32)\n",
    "    return y_hat, logits\n",
    "\n",
    "# 인풋 아웃풋 데이터, 드롭아웃 확률을 입력받기위한 플레이스홀더를 정의합니다.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환합니다.\n",
    "y_onehot = tf.one_hot(y, 10)\n",
    "\n",
    "# Convolutional Neural Networks(CNN) 그래프를 생성합니다.\n",
    "y_hat, logits = build_CNN_classifier(x)\n",
    "y_onehot = tf.one_hot(y, 10)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=logits))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_hat, y), tf.float32))\n",
    "\n",
    "lr = tf.placeholder(name='lr', shape=[], dtype=tf.float32)\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "# 모든 변수들을 초기화한다. \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "trainData, testData = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "trainData_by1Nid = fl_data.preprocess('cnn', 'cifar10', trainData, False)\n",
    "testData_by1Nid = fl_data.preprocess('cnn', 'cifar10', testData, False)\n",
    "x_train = trainData_by1Nid[0]['x']\n",
    "y_train = trainData_by1Nid[0]['y']\n",
    "x_test = testData_by1Nid[0]['x'][0:10000]\n",
    "y_test = testData_by1Nid[0]['y'][0:10000]\n",
    "print(x_train.shape)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "lr_ = 0.01\n",
    "\n",
    "# 10000 Step만큼 최적화를 수행합니다.\n",
    "for i in range(1000000):\n",
    "    batch = next_batch(BATCH_SIZE, x_train, y_train)\n",
    "\n",
    "    # 20% 확률의 Dropout을 이용해서 학습을 진행합니다.\n",
    "    sess.run(train_step, feed_dict={lr: lr_, x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    # 100 Step마다 training 데이터셋에 대한 정확도와 loss를 출력합니다.\n",
    "    if i % 390 == 0:\n",
    "        numTestIters = int(10000/BATCH_SIZE)\n",
    "        losses_ = [] ; accs_ = [] ; idxBegin = 0 ; idxEnd = 0\n",
    "        for _ in range(numTestIters):\n",
    "            idxEnd += BATCH_SIZE\n",
    "            sampleBatch_x = x_test[idxBegin:idxEnd]\n",
    "            sampleBatch_y = y_test[idxBegin:idxEnd]\n",
    "            (loss_, acc_) = sess.run((loss, accuracy), feed_dict={lr: lr_, x: sampleBatch_x, y: sampleBatch_y, keep_prob: 1.0})\n",
    "            losses_.append(loss_)\n",
    "            accs_.append(acc_)\n",
    "            idxBegin = idxEnd\n",
    "        (loss_, acc_) = np.mean(losses_), np.mean(accs_)\n",
    "        #vs = sess.run(tf.concat([ tf.reshape(layer, [-1]) for layer in tf.get_collection(tf.GraphKeys.VARIABLES) ], axis=0))\n",
    "        #numVars = vs.shape ; print(numVars)\n",
    "        print(int(i / 390), loss_, acc_)\n",
    "        \n",
    "        lr_ *= 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 32\n",
    "##### https://dnddnjs.github.io/cifar10/2018/10/09/resnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2NN Multi Layer Perceptron\n",
    "##### https://towardsdatascience.com/multi-layer-perceptron-using-tensorflow-9f3e218a4809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# iris = load_iris()\n",
    "# x_vals = iris.data[:, [0,3]]\n",
    "# y_vals = np.array([1 if y==0 else -1 for y in iris.target])\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_vals, y_vals, random_state=0, stratify=y_vals)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import fl_data\n",
    "\n",
    "ops.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "DATA_NAME = 'mnist-o' # mnist-o / mnist-f / cifar-10\n",
    "\n",
    "if DATA_NAME == 'mnist-o':\n",
    "    train_data, test_data = tf.keras.datasets.mnist.load_data()\n",
    "elif DATA_NAME == 'mnist-f':\n",
    "    train_data, test_data = tf.keras.datasets.fashion_mnist.load_data()\n",
    "elif DATA_NAME == 'cifar10':\n",
    "    train_data, test_data = tf.keras.datasets.cifar10.load_data()\n",
    "else:\n",
    "    raise Exception(DATA_NAME)\n",
    "train_dataBy1Nid = fl_data.preprocess('sr', DATA_NAME, train_data, True)\n",
    "test_dataBy1Nid = fl_data.preprocess('sr', DATA_NAME, test_data, True)\n",
    "print(train_dataBy1Nid[0]['x'].shape, test_dataBy1Nid[0]['x'].shape)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Placeholders for the input data\n",
    "x = tf.placeholder(name='x', shape=[None, 784], dtype=tf.float32)\n",
    "y = tf.placeholder(name='y', shape=[None], dtype=tf.int32)\n",
    "y_onehot = tf.one_hot(y, 10)\n",
    "\n",
    "num_features = 784\n",
    "num_layers_0 = 200\n",
    "num_layers_1 = 200\n",
    "#regularizer_rate = 0.1\n",
    "\n",
    "#keep_prob = tf.constant(0.6)\n",
    "\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "tf.random.set_random_seed(1234) # 모든 노드가 같은 Initial Random Seed 를 갖지 않으면 학습되지 않음 (FedAvg 논문 참조)\n",
    "W_0 = tf.get_variable('W_0', dtype=tf.float32, initializer=tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "b_0 = tf.get_variable('b_0', dtype=tf.float32, initializer=tf.random_normal([num_layers_0]))\n",
    "W_1 = tf.get_variable('W_1', dtype=tf.float32, initializer=tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "b_1 = tf.get_variable('b_1', dtype=tf.float32, initializer=tf.random_normal([num_layers_1]))\n",
    "W_2 = tf.get_variable('W_2', dtype=tf.float32, initializer=tf.random_normal([num_layers_1,10], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "b_2 = tf.get_variable('b_2', dtype=tf.float32, initializer=tf.random_normal([10]))\n",
    "\n",
    "## Initializing weigths and biases\n",
    "hidden_output_0 = tf.nn.relu(tf.matmul(x,W_0)+b_0)\n",
    "#hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0,W_1)+b_1)\n",
    "#hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "logits = tf.matmul(hidden_output_1, W_2) + b_2\n",
    "y_prob = tf.nn.softmax(logits)\n",
    "y_hat = tf.cast(tf.argmax(y_prob, 1), tf.int32)\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_onehot))# \\\n",
    "        #+ regularizer_rate*(tf.reduce_sum(tf.square(b_0)) + tf.reduce_sum(tf.square(b_1)))\n",
    "train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_hat, y), tf.float32))\n",
    "\n",
    "# initialize variable\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "x_train = train_dataBy1Nid[0]['x'][0:10000]\n",
    "y_train = train_dataBy1Nid[0]['y'][0:10000]\n",
    "x_test = test_dataBy1Nid[0]['x'][0:10000]\n",
    "y_test = test_dataBy1Nid[0]['y'][0:10000]\n",
    "\n",
    "# x = x_train\n",
    "# y = y_train.reshape(-1, 1)\n",
    "for t in np.arange(100000):\n",
    "    sess.run(train_op, feed_dict={x: x_train, y:y_train})\n",
    "    loss_ = sess.run(loss, feed_dict={x: x_train, y:y_train})\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: x_test, y:y_test})\n",
    "    if t % 100 == 0:\n",
    "        print('{},{},{}'.format(t, loss_, accuracy_))\n",
    "    if accuracy_ >= 0.99:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
