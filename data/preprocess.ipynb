{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28) (70000,)\n",
      "42000 14000 14000\n",
      "(70000, 28, 28) (70000,)\n",
      "42000 14000 14000\n",
      "(60000, 32, 32, 3) (60000,)\n",
      "36000 12000 12000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import fl_util\n",
    "\n",
    "def prepare(dataName, flatten=True):\n",
    "    if dataName == 'mnist-o':\n",
    "        trainData, testData = tf.keras.datasets.mnist.load_data()\n",
    "    elif dataName == 'mnist-f':\n",
    "        trainData, testData = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    elif dataName == 'cifar10':\n",
    "        trainData, testData = tf.keras.datasets.cifar10.load_data()\n",
    "    else:\n",
    "        raise Exception(DATA_NAME)\n",
    "    x = np.concatenate((trainData[0], testData[0]), axis=0)\n",
    "    y = np.concatenate((trainData[1], testData[1]), axis=0)\n",
    "    if dataName == 'cifar10':\n",
    "        y = y.flatten() # cifar10 의 경우 flatten 필요\n",
    "    print(x.shape, y.shape)\n",
    "        \n",
    "    # Normalize\n",
    "    x = np.array([ x_ / 255.0 for x_ in x ], dtype=np.float32)\n",
    "#     x = np.array([ x.flatten() / 255.0 if flatten else x / 255.0 for x in dataX ], dtype=np.float32)\n",
    "\n",
    "#     if modelName == 'svm':\n",
    "#         if dataName == 'cifar10':\n",
    "#             # airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "#             vehicleClasses = [0, 1, 8, 9]\n",
    "#             y = np.array([ -1 if y in vehicleClasses else 1 for y in dataY ], dtype=np.int32)\n",
    "#         else:\n",
    "#             raise Exception(dataName)\n",
    "#     else:\n",
    "#         y = np.array(dataY, dtype=np.int32)\n",
    "\n",
    "    # seed 를 고정시켜서 분할\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.4, random_state=1234)\n",
    "    x_val, x_test, y_val, y_test = model_selection.train_test_split(x_test, y_test, test_size=0.5, random_state=1234)\n",
    "    trainData_by1Nid = np.array([ { 'x': x_train, 'y': y_train } ])\n",
    "    valData_by1Nid = np.array([ { 'x': x_val, 'y': y_val } ])\n",
    "    testData_by1Nid = np.array([ { 'x': x_test, 'y': y_test } ])\n",
    "    print(len(trainData_by1Nid[0]['x']), len(valData_by1Nid[0]['x']), len(testData_by1Nid[0]['y']))\n",
    "    \n",
    "    fl_util.serialize(os.path.join(dataName, 'train'), trainData_by1Nid)\n",
    "    fl_util.serialize(os.path.join(dataName, 'val'), valData_by1Nid)\n",
    "    fl_util.serialize(os.path.join(dataName, 'test'), testData_by1Nid)\n",
    "\n",
    "# MNIST-O\n",
    "prepare('mnist-o')\n",
    "prepare('mnist-f')\n",
    "prepare('cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf (./preprocess.sh -s iid --sf 0.1 로 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78353, 784) (78353,)\n",
      "47011 15671 15671\n"
     ]
    }
   ],
   "source": [
    "dataName = 'femnist'\n",
    "uids, _, data = fl_util.readJsonDir(os.path.join(dataName, 'sampled'))\n",
    "x = np.concatenate([ data[uid]['x'] for uid in uids ], axis=0)\n",
    "y = np.concatenate([ data[uid]['y'] for uid in uids ], axis=0)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Normalize\n",
    "x = np.array([ x_ / 255.0 for x_ in x ], dtype=np.float32)\n",
    "\n",
    "# seed 를 고정시켜서 분할\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.4, random_state=1234)\n",
    "x_val, x_test, y_val, y_test = model_selection.train_test_split(x_test, y_test, test_size=0.5, random_state=1234)\n",
    "trainData_by1Nid = np.array([ { 'x': x_train, 'y': y_train } ])\n",
    "valData_by1Nid = np.array([ { 'x': x_val, 'y': y_val } ])\n",
    "testData_by1Nid = np.array([ { 'x': x_test, 'y': y_test } ])\n",
    "print(len(trainData_by1Nid[0]['x']), len(valData_by1Nid[0]['x']), len(testData_by1Nid[0]['y']))\n",
    "\n",
    "fl_util.serialize(os.path.join(dataName, 'train'), trainData_by1Nid)\n",
    "fl_util.serialize(os.path.join(dataName, 'val'), valData_by1Nid)\n",
    "fl_util.serialize(os.path.join(dataName, 'test'), testData_by1Nid)\n",
    "\n",
    "# data = {}\n",
    "# data['people'] = []\n",
    "# data['people'].append({\n",
    "#     'name': 'Scott',\n",
    "#     'website': 'stackabuse.com',\n",
    "#     'from': 'Nebraska'\n",
    "# })\n",
    "# data['people'].append({\n",
    "#     'name': 'Larry',\n",
    "#     'website': 'google.com',\n",
    "#     'from': 'Michigan'\n",
    "# })\n",
    "# data['people'].append({\n",
    "#     'name': 'Tim',\n",
    "#     'website': 'apple.com',\n",
    "#     'from': 'Alabama'\n",
    "# })\n",
    "\n",
    "# with open('data.txt', 'w') as outfile:\n",
    "#     json.dump(data, outfile)\n",
    "\n",
    "# with open('data.txt') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "#     for p in data['people']:\n",
    "#         print('Name: ' + p['name'])\n",
    "#         print('Website: ' + p['website'])\n",
    "#         print('From: ' + p['from'])\n",
    "#         print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
